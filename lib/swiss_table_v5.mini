import map;
import io;
import array;

// --- Control Byte Constants (unchanged) ---
def empty() -> i8 { return 0 as i8; }
def deleted() -> i8 { return -2 as i8; }
def occupied_mask() -> i8 { return 1 as i8; }
def group_width() -> i32 { return 16; }

class Entry[K,V] {
    @hash: i32
    @key: K
    @value: V

    getters @hash, @key, @value

    def init(@key: K, @value: V, @hash: i32) {}

    def to_pair() -> Pair[K, V] {
        return Pair{@key, @value};
    } ~> { ret.first == @key, ret.second == @value }
}

class SwissTable[K, V] extends HashMap[K, V] {
    @entries: Buffer[Entry[K,V]?]
    @metadata: Buffer[i8]
    @entries_len: i32
    @size: i32
    @hasher: (K) -> i32
    @eq: (K, K) -> Bool

    getters @size, @entries_len

    def Self.h2_to_control(hash: i32) -> i8 {
        h2_6bit = (hash >> 26) as i8;
        return (h2_6bit << 1) bit_or occupied_mask();
    }

    def init(@hasher: (K) -> i32, @eq: (K, K) -> Bool) {
        // Minimum capacity must be >= group_width for SIMD loads
        initial_capacity = 16;
        // Allocate extra group for safe SIMD loads near end
        @entries = Buffer[Entry[K,V]?].new(initial_capacity + group_width());
        @metadata = Buffer[i8].new(initial_capacity + group_width());
        @entries_len = initial_capacity;
        @size = 0;
    }

    // =========================================================
    // SIMD Group Operations - The core of Swiss Table performance
    // =========================================================

    // Load 16 control bytes and find slots matching h2_control
    // Returns a bitmask where bit i is set if metadata[group_start + i] matches
    def match_h2_in_group(group_start: i32, h2_control: i8) -> Tuple[16 x Bool] {
        group = @metadata.[16 from group_start];   // SIMD load: Tuple[16 x i8]
        needle = 16 of h2_control;                 // broadcast: Tuple[16 x i8]
        matches = group == needle;                 // compare: Tuple[16 x Bool]
        return matches;                     // extract bitmask
    }

    // Find empty slots in a group
    def match_empty_in_group(group_start: i32) -> Tuple[16 x Bool] {
        group = @metadata.[16 from group_start];
        empty_vec = 16 of empty();
        matches = group == empty_vec;
        return matches;
    }

    // Find sentinel slots (empty OR deleted) - slots where LSB is 0
    def match_sentinel_in_group(group_start: i32) -> Tuple[16 x Bool] {
        group = @metadata.[16 from group_start];
        mask_vec = 16 of occupied_mask();          // (1, 1, 1, ..., 1)
        lsb_bits = group bit_and mask_vec;         // extract LSB of each byte
        zero_vec = 16 of (0 as i8);
        is_sentinel = lsb_bits == zero_vec;        // LSB == 0 means sentinel
        return is_sentinel;
    }

    // =========================================================
    // Core lookup with SIMD acceleration
    // =========================================================

    def find_slot(key: K, hash: i32) -> i32 {
        capacity = @entries_len;
        mask = capacity - 1;
        h2_control = Self.h2_to_control(hash);
        probe_index = hash bit_and mask;

        first_tombstone = -1;
        entries = @entries;

        while true {
            // SIMD: Check all 16 slots in parallel for H2 match
            h2_matches = self.match_h2_in_group(probe_index, h2_control);

            // Iterate through H2 matches (usually 0-2 iterations)
            bit_pos = 0;
            for match in h2_matches {
                if not match {
                    bit_pos = bit_pos + 1;
                    continue;
                }
                slot = (probe_index + bit_pos) bit_and mask;
                entry = entries.[slot];
                if entry is Entry[K,V] {
                    if entry.hash() == hash and @eq.call(entry.key(), key) {
                        return slot;  // Found it!
                    }
                }
                bit_pos = bit_pos + 1;
            }

            // SIMD: Check for any empty slot (terminates probing)
            empty_matches = self.match_empty_in_group(probe_index);

            if empty_matches != (16 of false) {
                // Key doesn't exist - return insertion point
                if first_tombstone != -1 { return first_tombstone; }
                bit_pos = 0;
                for match in empty_matches {
                    if match { return (probe_index + bit_pos) bit_and mask; }
                    bit_pos = bit_pos + 1;
                }
            }

            // No empty slots - check for tombstones to potentially reuse
            if first_tombstone == -1 {
                // Sentinels that aren't empty must be tombstones
                tombstone_matches = self.match_sentinel_in_group(probe_index);
                bit_pos = 0;
                for match in tombstone_matches {
                    if match {
                        first_tombstone = (probe_index + bit_pos) bit_and mask;
                        break;
                    }
                    bit_pos = bit_pos + 1;
                }
            }

            // Triangular probing: probe_index += probe_count * group_width
            // Simplified here to linear for clarity
            probe_index = (probe_index + group_width()) bit_and mask;
        }
    }

    // Optimized path for insertion during resize (no existing keys)
    def find_empty_slot(hash: i32) -> i32 {
        capacity = @entries_len;
        mask = capacity - 1;
        probe_index = hash bit_and mask;

        while true {
            sentinel_matches = self.match_sentinel_in_group(probe_index);

            if sentinel_matches != (16 of false) {
                bit_pos = 0;
                for match in sentinel_matches {
                    if match {
                        return (probe_index + bit_pos) bit_and mask;
                    }
                    bit_pos = bit_pos + 1;
                }
            }

            probe_index = (probe_index + group_width()) bit_and mask;
        }
    }

    // =========================================================
    // Public API
    // =========================================================

    def insert(key: K, value: V) {
        self.ensure_capacity(@size + 1);
        hash = @hasher.call(key);
        slot = self.find_slot(key, hash);

        is_new = (@metadata.[slot] bit_and occupied_mask()) == 0;
        if is_new { @size = @size + 1; }

        @metadata.[slot] = Self.h2_to_control(hash);
        @entries.[slot] = Entry[K,V].new(key, value, hash);
    }

    def get(key: K) -> V? {
        if @size == 0 { return nil; }
        hash = @hasher.call(key);
        slot = self.find_slot(key, hash);

        if (@metadata.[slot] bit_and occupied_mask()) == 0 { return nil; }

        entry = @entries.[slot];
        if entry is Nil { return nil; }
        return entry.value();
    }

    def remove(key: K) -> V? {
        if @size == 0 { return nil; }
        hash = @hasher.call(key);
        slot = self.find_slot(key, hash);

        if (@metadata.[slot] bit_and occupied_mask()) == 0 { return nil; }

        entry = @entries.[slot];
        if entry is Nil { return nil; }

        val = entry.value();
        @metadata.[slot] = deleted();
        @entries.[slot] = nil;
        @size = @size - 1;
        return val;
    }

    def ensure_capacity(required: i32) {
        if required <= (@entries_len * 7) / 8 { return; }

        new_capacity = @entries_len * 2;
        if new_capacity < 16 { new_capacity = 16; }
        self.resize(new_capacity);
    }

    def resize(new_capacity: i32) {
        old_entries = @entries;
        old_metadata = @metadata;
        old_len = @entries_len;

        // Allocate with padding for safe SIMD loads
        @entries = Buffer[Entry[K,V]?].new(new_capacity + group_width());
        @metadata = Buffer[i8].new(new_capacity + group_width());
        @entries_len = new_capacity;
        @size = 0;

        for i in 0..old_len {
            if (old_metadata.[i] bit_and occupied_mask()) == 0 { continue; }
            entry = old_entries.[i];
            if entry is Nil { continue; }

            slot = self.find_empty_slot(entry.hash());
            @entries.[slot] = entry;
            @metadata.[slot] = Self.h2_to_control(entry.hash());
            @size = @size + 1;
        }
    }

    def clear() {
        initial_capacity = 16;
        @entries = Buffer[Entry[K,V]?].new(initial_capacity + group_width());
        @metadata = Buffer[i8].new(initial_capacity + group_width());
        @entries_len = initial_capacity;
        @size = 0;
    }

    // return (new_cursor, next_entry) in buffer starting at position of input cursor
    def scan_next(start_cursor: i32) -> Pair[i32, Pair[K,V]]? {
        meta = @metadata;
        entries = @entries;

        if start_cursor >= @entries_len { return nil; }

        for cursor in start_cursor..@entries_len {
            if (meta.[cursor] bit_and occupied_mask()) == 0 { continue; }
            entry = entries.[cursor];
            if entry is Nil { continue; }
            return Pair{cursor + 1, entry.to_pair()};
        }
        return nil;
    } ~> { ret holds @elems_reg }

    def iterator() -> SwissTableIterator[K,V] {
        return SwissTableIterator{self};
    } ~> { ret.map == self }

}

class SwissTableIterator[K,V] extends Iterator[Pair[K,V]] {
    @map : SwissTable[K,V]
    @cursor: i32

    regions { @elems_reg == @map.elems_reg }

    def init(@map : SwissTable[K,V]) {
        @cursor = 0;
    }

    def next() -> Pair[K, V]? {
        tuple_or_nil = @map.scan_next(@cursor);
        if tuple_or_nil is Nil { return nil; }
        (@cursor, pair) = tuple_or_nil;
        return pair;
    }
}