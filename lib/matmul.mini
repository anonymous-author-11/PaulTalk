import std;

// records time in milliseconds
extern def clock() -> i64

// compiles to:
// define i32 @min(i32 %0, i32 %1) {
//   %3 = call i32 @llvm.smin.i32(i32 %0, i32 %1)
//   ret i32 %3
// }
def min(a : i32, b : i32) -> i32 {
    if a < b { return a; }
    return b;
}

// compiles to:
// define <8 x double> @simd_add(<8 x double> %0, <8 x double> %1) {
//   %3 = add <8 x double> %1, %0
//   ret <8 x double> %3
// }
def simd_add(a : Tuple[8 x f64], b : Tuple[8 x f64]) -> Tuple[8 x f64] {
    return (a.[0] + b.[0], a.[1] + b.[1], a.[2] + b.[2], a.[3] + b.[3], a.[4] + b.[4], a.[5] + b.[5], a.[6] + b.[6], a.[7] + b.[7]);
}

// compiles to:
// define <8 x double> @simd_mul(<8 x double> %0, <8 x double> %1) {
//   %3 = mul <8 x double> %1, %0
//   ret <8 x double> %3
// }
def simd_mul(a : Tuple[8 x f64], b : Tuple[8 x f64]) -> Tuple[8 x f64] {
    return (a.[0] * b.[0], a.[1] * b.[1], a.[2] * b.[2], a.[3] * b.[3], a.[4] * b.[4], a.[5] * b.[5], a.[6] * b.[6], a.[7] * b.[7]);
}

class Matrix {
    @data : Buffer[f64]
    @rows : i32
    @cols : i32

    // A subtype could override this to do something adaptive

    // L1_TILE_SIZE (Micro-kernel tile size, aims for L1 cache and register reuse)
    // This was your original block_size
    def Self.l1_tile_size() -> i32 { return 32; } 

    // L2_TILE_SIZE (Macro-kernel tile size, aims for L2 cache reuse)
    // This should be larger, e.g., 128 or 256. 
    // Must be a multiple of l1_tile_size() for cleaner loop bounds, or handle remainders.
    // Let's choose 128 for this example.
    def Self.l2_tile_size() -> i32 { return 128; } 

    def init(@rows : i32, @cols : i32) {
        @data = Buffer[f64].new(@rows * @cols);
    }

    def data() -> Buffer[f64] { return @data; }
    def rows() -> i32 { return @rows; }
    def cols() -> i32 { return @cols; }
    
    def row_idx() -> Range { return 0..@rows; }
    def col_idx() -> Range { return 0..@cols; }

    // Because overflow *is defined*, it's imperative to cast to i64 *before* computing indices
    def get(row : i32, col : i32) -> f64 {
        index = (col as i64) * @rows + row;
        return @data.[index];
    }

    def set(row : i32, col : i32, value : f64) {
        index = (col as i64) * @rows + row;
        @data.[index] = value;
    }

    // relevant IR:
    // %31 = load <8 x double>, ptr %30, align 8
    // ret <8 x double> %31
    def get_simd(start_row : i32, col : i32) -> Tuple[8 x f64] {
        start_index = (col as i64) * @rows + start_row;
        data = @data;
        a = data.[start_index + 0];
        b = data.[start_index + 1];
        c = data.[start_index + 2];
        d = data.[start_index + 3];
        e = data.[start_index + 4];
        f = data.[start_index + 5];
        g = data.[start_index + 6];
        h = data.[start_index + 7];
        return (a, b, c, d, e, f, g, h);
    }

    // relevant IR:
    // store <8 x double> %5, ptr %31, align 8
    // ret void
    def set_simd(start_row : i32, col : i32, values : Tuple[8 x f64]) {
        start_index = (col as i64) * @rows + start_row;
        data = @data;
        data.[start_index + 0] = values.[0];
        data.[start_index + 1] = values.[1];
        data.[start_index + 2] = values.[2];
        data.[start_index + 3] = values.[3];
        data.[start_index + 4] = values.[4];
        data.[start_index + 5] = values.[5];
        data.[start_index + 6] = values.[6];
        data.[start_index + 7] = values.[7];
    }

    def *(other : Matrix) -> Matrix {
        if @cols != other.rows() { yield(Exception.new("Incompatible matrix dimensions for multiplication")); }
        
        result = Matrix.new(@rows, other.cols());
        
        m = @rows;    // Rows of self (A) and result (C)
        p = @cols;    // Cols of self (A) and rows of other (B)
        n = other.cols(); // Cols of other (B) and result (C)

        l1_ts = Self.l1_tile_size(); // e.g., 32
        l2_ts = Self.l2_tile_size(); // e.g., 128

        // Outer loops: Iterate by L2_TILE_SIZE (e.g., 128)
        // Loop order jc, kc, ic (common in BLAS-like libraries)
        // jc: iterates over columns of C and B
        // kc: iterates over the common dimension P
        // ic: iterates over rows of C and A
        for jc in (0..n).step(l2_ts) { // Iterate N by l2_ts
            jc_end = min(jc + l2_ts, n);
            for kc in (0..p).step(l2_ts) { // Iterate P by l2_ts
                kc_end = min(kc + l2_ts, p);
                for ic in (0..m).step(l2_ts) { // Iterate M by l2_ts
                    ic_end = min(ic + l2_ts, m);

                    // Inner loops: Iterate by L1_TILE_SIZE (e.g., 32)
                    // These are your original loops, but now bounded by the L2 tile (ic, jc, kc extents)
                    // Original loop order: jj, kk, ii
                    for jj_l1 in (jc..jc_end).step(l1_ts) { // Iterate current L2 N-slice by l1_ts
                        jj_l1_end = min(jj_l1 + l1_ts, jc_end);
                        for kk_l1 in (kc..kc_end).step(l1_ts) { // Iterate current L2 P-slice by l1_ts
                            kk_l1_end = min(kk_l1 + l1_ts, kc_end);
                            for ii_l1 in (ic..ic_end).step(l1_ts) { // Iterate current L2 M-slice by l1_ts
                                ii_l1_end = min(ii_l1 + l1_ts, ic_end);

                                // Micro-kernel: Operates on an l1_ts x l1_ts (conceptual) block
                                // This is your original innermost logic for a block
                                // Bounds are:
                                // C_rows: ii_l1 to ii_l1_end
                                // C_cols: jj_l1 to jj_l1_end
                                // K_dim:  kk_l1 to kk_l1_end

                                for j_block in jj_l1..jj_l1_end { // Iterate columns within L1 C-tile
                                    
                                    i_block = ii_l1;
                                    // Vectorized part for rows of C
                                    while i_block <= ii_l1_end - 8 { // SIMD_WIDTH = 8
                                        // Load C_vec for C[i_block..i_block+7, j_block]
                                        // It's an accumulator, so it's loaded once per (i_block, j_block) pair
                                        // and updated over the k_block loop.
                                        c_vec = result.get_simd(i_block, j_block);

                                        for k_block in kk_l1..kk_l1_end { // Iterate K within L1 tile
                                            b_kj = other.get(k_block, j_block); // B[k, j]
                                            if b_kj == 0.0 { continue; } // Sparsity check

                                            // Load A_vec for A[i_block..i_block+7, k_block]
                                            a_vec = self.get_simd(i_block, k_block); // A[i, k]
                                            
                                            // Broadcast b_kj to a SIMD vector
                                            // (Assuming compiler can optimize this or you have an intrinsic)
                                            b_broadcast_vec = (b_kj, b_kj, b_kj, b_kj, b_kj, b_kj, b_kj, b_kj);
                                            
                                            // FMA: c_vec = (a_vec * b_broadcast_vec) + c_vec
                                            c_vec = simd_add(simd_mul(a_vec, b_broadcast_vec), c_vec);
                                        }
                                        result.set_simd(i_block, j_block, c_vec); // Store accumulated C_vec
                                        i_block = i_block + 8;
                                    }
                                    
                                    // Scalar cleanup for remaining rows in the L1 C-tile
                                    while i_block < ii_l1_end {
                                        c_val = result.get(i_block, j_block);
                                        for k_block in kk_l1..kk_l1_end {
                                            b_kj = other.get(k_block, j_block);
                                            // Note: Original code had a_val = self.get(i_block, k_block);
                                            // This is correct for C(i,j) += A(i,k) * B(k,j)
                                            a_val = self.get(i_block, k_block); 
                                            c_val = c_val + a_val * b_kj;
                                        }
                                        result.set(i_block, j_block, c_val);
                                        i_block = i_block + 1;
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        return result;
    }
}

def benchmark_matrix_multiplication() {
    size = 4096;

    matrix1 = Matrix.new(size, size);
    matrix2 = Matrix.new(size, size);
    
    for i in 0..size {
        for j in 0..size {
            val1 = (i + j) % 10;
            val2 = (i - j + 10) % 10;
            matrix1.set(i, j, val1 as f64); 
            matrix2.set(i, j, val2 as f64);
        }
    }
    
    start_time = clock();
    result = matrix1 * matrix2;
    end_time = clock();
    
    time_taken = end_time - start_time;
    
    IO.print("Matrix multiplication");
    IO.print(size);
    IO.print("x");
    IO.print(size);
    IO.print("Time taken:");
    IO.print(time_taken);
    
    sum = 0.0;
    for i in 0..size { sum = sum + result.get(0, i); }
    IO.print("Sum of first row (for verification):");
    IO.print(sum);
}

benchmark_matrix_multiplication();
